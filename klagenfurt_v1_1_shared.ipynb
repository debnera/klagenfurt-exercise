{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6rvc_vRRE9q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise: Predicting Power Consumption in a Kubernetes Cluster**\n",
        "\n",
        "This exercise will guide you through analyzing a dataset obtained from a Kubernetes cluster and building a predictive model for power consumption. The steps include:\n",
        "\n",
        "1. **Load and Preprocess Data:** Load a dataset containing various metrics from a Kubernetes cluster and clean it by removing static or missing values.\n",
        "2. **Explore Data:** Identify relevant features related to power consumption using string matching and correlation analysis.\n",
        "3. **Visualize Data:** Plot power consumption trends and relationships between different features.\n",
        "4. **Build a Predictive Model:** Train a linear regression model to predict power consumption based on selected features.\n",
        "5. **Evaluate Model Performance:** Assess the accuracy of your model using mean squared error and visualization.\n"
      ],
      "metadata": {
        "id": "MPee9Fd5RKSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Prepare the dataset\n",
        "\n",
        "Steps:\n",
        "1. Download the dataset\n",
        "2. Load the dataset as a Pandas dataframe\n",
        "3. Remove static values"
      ],
      "metadata": {
        "id": "l1xiAvsXY3_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 1.1: Download the dataset from Google Drive\n",
        "\n",
        "- The downloaded file should appear on the file-browser on the left-side menu\n",
        "- This dataset includes data from one of the worker nodes in the cluster (worker 1)\n",
        "\n",
        "TODO: Nothing, everything is included.\n",
        "\"\"\"\n",
        "!gdown https://drive.google.com/uc?id=1Lyl26JCUmXs0IRdURfVd9j4If5xnq5O3  # worker1.feather"
      ],
      "metadata": {
        "id": "1rQWgc00S9Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 1.2: Load dataframe and print info\n",
        "\n",
        "TODO: Nothing, everything is included.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "path = \"/content/worker1.feather\"\n",
        "original_df = pd.read_feather(path)\n",
        "\n",
        "# Count number of rows and cols in the original df\n",
        "print(f\"Loaded {len(original_df)} rows and {len(original_df.columns)} columns\")"
      ],
      "metadata": {
        "id": "D8a-Hp8yTBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 1.3: Remove nans and static columns\n",
        "\n",
        "- There are almost 16 000 columns in the dataset, however a significant portion of the data is useless or missing\n",
        "- Removing columns with mostly static data will make it much easier to handle the dataset\n",
        "\n",
        "TODO: Fill the missing code (very obvious hints in comments)\n",
        "\"\"\"\n",
        "import random\n",
        "\n",
        "# Make a copy of the dataframe, so we do not need to keep loading from file after every change\n",
        "df = original_df.copy()\n",
        "\n",
        "# To remove unnecessary columns, we need to figure out which columns are useless\n",
        "static_columns = [] # TODO: fill this list with the names of the columns (see hint below)\n",
        "\n",
        "# HINT: For example, first count the number of unique values in each column\n",
        "# unique_counts = df.nunique()\n",
        "# Then find all columns with less than two unique values\n",
        "# static_columns = unique_counts[unique_counts <= 2].index\n",
        "\n",
        "# Remove the static columns from the dataframe\n",
        "df = df.drop(static_columns, axis=1)\n",
        "print(f\"Removing {len(static_columns)} static columns ({len(df.columns)} remaining)\")\n",
        "\n",
        "# Print some examples of the removed columns\n",
        "indices = [x for x in range(len(static_columns))]\n",
        "for i in range(10):\n",
        "  col = static_columns[i]\n",
        "  vals = original_df[col].unique()  # Get a set containing all values in this column\n",
        "  print(f\"Removed: {col}, valuess: {vals}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FQigZRgHTJKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 1.4 Explore the dataset\n",
        "\n",
        "TODO: Have a look at the dataset.\n",
        "\n",
        "- What do the columns look like?\n",
        "- What do the values look like?\n",
        "- How much data is there?\n",
        "\"\"\"\n",
        "\n",
        "# for example, you can print a truncated list of all columns\n",
        "print(f\"{len(df)} rows and {len(df.columns)} columns\")\n",
        "print(df.columns)\n",
        "\n",
        "df\n",
        "\n",
        "# TODO: print values from one column, plot a column, print all 600+ columns, etc, ...\n",
        "\n",
        "# for col in df.columns:\n",
        "#   print(col)"
      ],
      "metadata": {
        "id": "w-WcvHe8ZXQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Plot power consumption over time\n",
        "\n",
        "Our goal in this step is to find out how to plot the power consumption of this worker node. In order to do this, we need to first figure out the columns that represent the power usage.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Find the columns representing power usage\n",
        "2. Process the columns into a more usable format\n",
        "3. Plot the columns"
      ],
      "metadata": {
        "id": "-CZiachBYEbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 2.1 In order to plot the power consumption of this worker node, we need to first\n",
        "figure out the relevant columns.\n",
        "\n",
        "A) As a starting point, look at the Kepler's documentation for potentially relevant metrics:\n",
        "https://sustainable-computing.io/design/metrics/\n",
        "\n",
        "hint 1: CPU is likely using the most power in our cluster\n",
        "hint 2: What do terms like 'core', 'uncore' and 'package' mean in the context of a CPU?\n",
        "hint 3: Do we want information about 'containers' or 'node'?\n",
        "hint 4: You will likely find multiple metrics with quite similar values,\n",
        "        and it might be useful to plot multiple different metrics\n",
        "\n",
        "B) As the Kepler's documentation does not fully match the column names in the dataset,\n",
        "   we can use string matching to find columns that are similar to some keywords.\n",
        "\"\"\"\n",
        "\n",
        "# We can use difflib to search for close matches for a string. (Useful, since the column names are very verbose)\n",
        "import difflib\n",
        "keywords = '' # TODO: Replace with relevant keywords to search for (you can use multiple words separated by space or underscore)\n",
        "\n",
        "n = 6  # Maximum number of matches to return --- (use smaller values to decrease garbage results)\n",
        "cutoff = 0.05  # Minimum similarity for a match (0.0 to 1.0)  --- (use higher values to decrease garbage results)\n",
        "closest_matches = difflib.get_close_matches(keywords, df.columns, n = n,  cutoff=cutoff)\n",
        "\n",
        "# Print the matches\n",
        "if len(closest_matches) == 0:\n",
        "  print(\"No matches found! (did you include any keywords to search for?)\")\n",
        "for match in closest_matches:\n",
        "  print(match)"
      ],
      "metadata": {
        "id": "LVoSBreKWDN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 2.2 Let's plot these columns. However, it is very likely that resulting plots are not very informative as such.\n",
        "\n",
        "- Just running the code can be enough, unless you want to fine-tune the plots.\n",
        "\n",
        "Documentation for plotting pandas dataframes:\n",
        "https://pandas.pydata.org/docs/user_guide/visualization.html\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n",
        "\"\"\"\n",
        "from matplotlib import pyplot as plt\n",
        "df[closest_matches].plot()\n",
        "plt.xlabel('x_axis')\n",
        "plt.ylabel('y_axis')\n",
        "plt.title(\"Closest matches\")"
      ],
      "metadata": {
        "id": "-DPetxuAd_6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 2.3 Let's make the metrics more readable.\n",
        "\n",
        "We will do this by converting them from counters to gauges.\n",
        "\n",
        "For example, instead of getting the 'total joules consumed' we get 'joules per second'.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" 2.3.1 get the update interval of the dataset. Prometheus polls data from Kubernetes pods every n seconds. \"\"\"\n",
        "timestamp = df[\"timestamp\"]  # Timestamp column includes timestamps in seconds. It should be sorted in an ascending format.\n",
        "interval =  #  TODO: Compute the interval using the first two timestamps. This is usually between 5 to 15 seconds in our datasets (static value)\n",
        "\n",
        "\"\"\" 2.3.2: Use the update interval to convert counters to gauges \"\"\"\n",
        "rate_metric_list = []\n",
        "for metric in closest_matches:\n",
        "  timeseries = df[metric]\n",
        "  # TODO: Compute the change over time for the timeseries\n",
        "  # Hint 1: Compute the difference between consecutive rows of the timeseries (e.g., [row1 - row0, row2 - row1, ...])\n",
        "  # Hint 2: Use diff() from Pandas to do this\n",
        "  difference =  # TODO: Compute the change over time for the timeseries\n",
        "  # TODO: Compute rate per second (hint: remember the interval we computed above)\n",
        "  per_second = # TODO (you can do element-wise divisions on dataframes, e.g., 'df = df/2')\n",
        "  rate_metric_list.append(per_second)\n",
        "\n",
        "# Create a new dataframe from the gauges\n",
        "rates_df = pd.concat(rate_metric_list, axis=1)\n",
        "# rates_df  # (Uncomment this to visualize the new dataframe)"
      ],
      "metadata": {
        "id": "0jQBbDosf63j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 2.4 Let's plot these new metrics. They should now be more informative than before.\n",
        "\n",
        "- Just running the code can be enough, unless you want to fine-tune the plots.\n",
        "- If you did everything correctly in tasks 2.1 - 2.3, you should see six clear peaks in the power data\n",
        "  (This is the day-night cycle from simulating a city for 6 days)\n",
        "- Max power usage for our cluster is around 24 Watts per worker.\n",
        "- The workload used for this dataset might not reach 24 Watts, but may be closer to 10 Watts.\n",
        "\n",
        "Documentation for plotting pandas dataframes:\n",
        "https://pandas.pydata.org/docs/user_guide/visualization.html\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n",
        "\"\"\"\n",
        "from matplotlib import pyplot as plt\n",
        "rates_df.plot()\n",
        "plt.xlabel('x_axis')\n",
        "plt.ylabel('y_axis')\n",
        "plt.title(\"Closest matches (converted to gauges)\")"
      ],
      "metadata": {
        "id": "isMmHMRej7Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Compute correlations\n",
        "\n",
        "Our cluster is currently a bare-metal cluster. This means that we can directly access the power consumption metric through APIs, such as RAPL.\n",
        "\n",
        "However, cloud servers are often virtualized with no access to these hardware level metrics. In this case, we may need to predict the power consumption from other metrics.\n",
        "\n",
        "In this phase we will train a power model that could be used to predict the power usage, even if we turn our cluster into a virtualized cluster. As the resulting power model is trained on data from our cluster hardware, the end result will only be capapble for doing predictions for similar hardware.\n",
        "\n",
        "Before we can train a model, we need to figure out the target column (e.g., power) and the input columns (i.e., which columns could be useful for predicting the power usage?).\n",
        "\n",
        "Tasks:\n",
        "1. Get the target column (e.g., the power consumption)\n",
        "2. Figure out potential candidates for input columns by computing correlations against the target column"
      ],
      "metadata": {
        "id": "DN9O01AQqQ0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 3.1: Get the target column (e.g., the power consumption)\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Get the target value for training power models (our target is the power)\n",
        "# Hint 1: Which column was the best for predicting the power in Phase 2\n",
        "# Hint 2: Remember to convert to a gauge\n",
        "# Hint 3: You can reuse a variable from previous tasks, or use the difflib again\n",
        "keywords = \"\"\n",
        "closest_matches = difflib.get_close_matches(keywords, df.columns, n = 1,  cutoff=0.05)\n",
        "print(closest_matches)\n",
        "power = df[closest_matches[0]].diff() / interval # TODO: Replace me with the power column instead of the timestamp\n",
        "power.plot()\n"
      ],
      "metadata": {
        "id": "Pjb4gEZtKR9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 3.2: Compute correlations againts the power\n",
        "\"\"\"\n",
        "# Power (of course) correlates with power - however the idea is to predict power when we do not have power metrics\n",
        "# -> Remove all power metrics from the dataset\n",
        "joule_columns = [x for x in df.columns if \"joules\" in x]\n",
        "non_joule_df = df.drop(joule_columns, axis=1)\n",
        "\n",
        "# Compute correlations against power\n",
        "# Hint: You can use https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html\n",
        "correlations =  # TODO: replace with correlations (dictionary of column-correlation pairs)\n",
        "\n",
        "# Sort and print some of the correlations\n",
        "correlations = correlations.sort_values(ascending=False)\n",
        "best_correlations = []\n",
        "for i, corr in enumerate(correlations.items()):\n",
        "  print(corr)\n",
        "  best_correlations.append(corr[0])\n",
        "  if i > 10:\n",
        "    break\n",
        "\n",
        "  #print(sorted(correlations, reverse=True))"
      ],
      "metadata": {
        "id": "-d2CjGnqjz6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgsYT94-gIgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4 (optional): Compute more correlations\n",
        "\n",
        "Similar to power, many metrics are included as counters and are therefore not useful for predicting the power consumption.\n",
        "\n",
        "Transforming these metrics to gauges could reveal more useful metrics that correlate with power consumption\n",
        "\n",
        "Hints:\n",
        "- How to figure out which metrics are counters that need to be?"
      ],
      "metadata": {
        "id": "XJvAJQbjqXgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: similar to Phase 3, but convert all counters to gauges before computing correlations\n",
        "# Hint: There is no strictly correct approach for finding out which metrics are counters in the first place.\n",
        "#       For example, you can try manually looking at the columns to try to find a pattern.\n",
        "# Hint 2: This task is completely optional.\n",
        "optional_correlations = []\n",
        "optional_best_correlations = []"
      ],
      "metadata": {
        "id": "bKHX7Sh2qPdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 5: Create a predictive model for power\n",
        "\n",
        "Now that we have the target column (power) and the input columns, we can try to train a model.\n",
        "\n",
        "Here we are using scikit-learn (sklearn) to train a very simple LinearRegression model. However, you could try to train more complex models in a very similar way by using the other regression algorithms from sklearn."
      ],
      "metadata": {
        "id": "zEbaFc6CrN0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 5.1: Fill the missing lines to train a LinearRegression model\n",
        "\n",
        "- How much is the mean square error? Can you improve it?\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Prepare the data\n",
        "# X is a dataframe containing the input features\n",
        "# y is a series/dataframe containing the target value that we want to predict (i.e., power)\n",
        "X = # TODO: Use the best correlating columns as the input\n",
        "y = # TODO: Use the power column as the target\n",
        "\n",
        "print(f\"Input columns: {X.columns}\")\n",
        "\n",
        "# Check for NaN values in the dataframes (sklearn does not like NaNs)\n",
        "print(\"NaN values in X:\", X.isna().sum().sum())\n",
        "print(\"NaN values in y:\", y.isna().sum())\n",
        "\n",
        "# Replace NaN values with 0 as a quick and dirty workaround\n",
        "X = X.fillna(0)\n",
        "y = y.fillna(0)\n",
        "\n",
        "# TODO: Split the data into training and testing sets (hint: train_test_split can do this for you)\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "X_train, X_test, y_train, y_test = # TODO: Use train_test_split-method, perhaps with 20% test data and some fixed random seed\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "caciNDF3O2UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 6: Assess the quality of the power model\n",
        "\n",
        "In addition to computing the MSE, we can evaluate the model by plotting the predictions againts real values."
      ],
      "metadata": {
        "id": "x0gx3V57rSik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 6.1: Plot predictions against the test set by running the code\n",
        "\n",
        "- What can you say from the plots?\n",
        "- How well are the predictions matching the real values?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot predicted vs real power\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.xlabel(\"Real Power\")\n",
        "plt.ylabel(\"Predicted Power\")\n",
        "plt.title(\"Real Power vs Predicted Power\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Add a diagonal line for reference\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NkBABjw3SiHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task 6.2: Plot predictions against the full dataset by running the code\n",
        "\n",
        "- What can you say from the plots?\n",
        "- How well are the predictions matching the real values?\n",
        "- What is the difference to the above plot?\n",
        "\"\"\"\n",
        "# Make predictions on the FULL training set (not just test set)\n",
        "y_pred_full = model.predict(X)\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "plot_df = pd.DataFrame({\n",
        "    'Real Power': y,\n",
        "    'Predicted Power': y_pred_full\n",
        "}, index=X.index)\n",
        "\n",
        "# Sort the DataFrame by index to avoid jumps in the plot\n",
        "plot_df = plot_df.sort_index()\n",
        "\n",
        "# Plot predicted vs real power\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(plot_df.index, plot_df['Real Power'], label='Real Power')\n",
        "plt.plot(plot_df.index, plot_df['Predicted Power'], label='Predicted Power')\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Power\")\n",
        "plt.title(\"Real Power vs Predicted Power (Full Training Set)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p82Z4pvUSp8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What next?\n",
        "\n",
        "For next step, you can try to evaluate your model against previously unseen data from other datasets:\n",
        "\n",
        "- *List of datasets here* (hopefully updated for the Thursday session)\n",
        "\n",
        "You can also try to improve the model by using more advanced approaches:\n",
        "\n",
        "- Use something other that LinearRegression\n",
        "- Use AutoML to automatically try different regression methods and feature selection methods\n",
        "- Do additional filtering and preprocessing of features"
      ],
      "metadata": {
        "id": "WQuYJZRRTYBz"
      }
    }
  ]
}