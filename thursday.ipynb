{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6rvc_vRRE9q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 7: Extension to previous exercise\n",
        "\n",
        "\n",
        "\n",
        "In the previous exercise, you trained and tested a simple power model againts one dataset.\n",
        "\n",
        "This notebook includes links to more data and a small script to test your model againts these datasets.\n",
        "\n",
        "You can copy-paste the code from this notebook to the previous exercise and try to evaluate your model.\n",
        "\n",
        "Steps:\n",
        "1. Copy-and-paste this code to the end of the previous exercise (after training your model)\n",
        "2. Download the new datasets using the provided code (only do this once)\n",
        "3. Run the second part of this code to evaluate your model againts these datasets\n",
        "4. Think about the following questions:\n",
        "\n",
        "- How well does your model generalize to the other datasets?\n",
        "- Are there differences between the workers?\n",
        "- Can you improve your model?\n",
        "  - e.g., use data from more datasets to train the model\n",
        "  - Try selecting better input features (e.g., something better than the simple correlations)\n",
        "  - Use more advanced ML algorithms (e.g., LinearRegression vs PolynomialRegression vs ... vs deep learning)\n",
        "  - Use autoML to automatically try different approaches\n",
        "  - ...?"
      ],
      "metadata": {
        "id": "MPee9Fd5RKSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "7.1: Download another dataset, including data from all workers.\n",
        "\n",
        "This dataset includes the six peaks of the simulated day-night cycle (less vehicles at night, more during day).\n",
        "\n",
        "During peak hours, the maximum amount of images recieved from vehicles and processed in the cluster was limited to 4mbps.\n",
        "\"\"\"\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1XCDQt5m7k4NuLn30iFZxENWP6hzyc2t0 -O mbps4_worker1.feather\n",
        "!gdown https://drive.google.com/uc?id=105QzBeUda6FfovMqa7916hV4zLGS-hGI -O mbps4_worker2.feather\n",
        "!gdown https://drive.google.com/uc?id=1uMBCK4PywMJaU0vhD2dHD_UFyxmQiofU -O mbps4_worker3.feather\n",
        "!gdown https://drive.google.com/uc?id=1Fe_atHnsOmAHckTLNVjlrPnwk3J7-eZF -O mbps4_worker4.feather\n",
        "!gdown https://drive.google.com/uc?id=1aitUG9j48i9JHJmfc-1vXjVVRdQZxc9J -O mbps4_worker5.feather\n",
        "\n",
        "\"\"\"\n",
        "Additionally, you can uncomment the following code to download yet another dataset.\n",
        "\n",
        "This dataset does not have the six peaks of the simulated day-night cycle (less vehicles at night, more during day). Instead, the workload increases linearly over time.\n",
        "\n",
        "It is very likely that your model will not generalize to this dataset.\n",
        "\"\"\"\n",
        "\n",
        "#!gdown https://drive.google.com/uc?id=1Yj6bFut4HE3BSqiM7aJKbXkq2I8LBCQ6 -O linear_worker1.feather\n",
        "#!gdown https://drive.google.com/uc?id=1YfbmnDAJY2XGVf5BTltBy8OEaPRX7_8e -O linear_worker2.feather\n",
        "#!gdown https://drive.google.com/uc?id=1zbGVwmXy8Y3JOaZMO3jXKcbPQ3zvpjMB -O linear_worker3.feather\n",
        "#!gdown https://drive.google.com/uc?id=1CIagdefkqgx3QJclR5ExhKg_J_zymjhn -O linear_worker4.feather\n",
        "#!gdown https://drive.google.com/uc?id=1c8tI7-6jSI3hOHFDQspZP-b-POPHRfqf -O linear_worker5.feather\n"
      ],
      "metadata": {
        "id": "6TWzDETmBDRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "7.2 Load the new datasets to a list, so you do not need to keep reloading them after every change to later code.\n",
        "\"\"\"\n",
        "\n",
        "evaluation_dfs = []\n",
        "for i in range(1, 6):\n",
        "  # Get the dataset\n",
        "  worker_df = pd.read_feather(f\"/content/mbps4_worker{i}.feather\")\n",
        "  # Remove the static columns from the dataframe (hopefully makes dealing with the dataset faster)\n",
        "  unique_counts = worker_df.nunique()\n",
        "  static_columns = unique_counts[unique_counts <= 2].index\n",
        "  worker_df = worker_df.drop(static_columns, axis=1)\n",
        "  evaluation_dfs.append(worker_df)"
      ],
      "metadata": {
        "id": "GJRO04lvPv_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "7.3 Evaluate your model against all workers from the new datasets\n",
        "\n",
        "NOTE: If your model relies on any data from specific containers, the evaluation will fail.\n",
        "      You cannot expect that the other workers are running the exact same containers with the exact same ID.\n",
        "      You will have to remove all container specific features from the feature candidates.\n",
        "\n",
        "- Can you improve the model to get better results?\n",
        "-- e.g., use better input features,\n",
        "      use better regression methods (check scikit-learn documentation or ask gemini on colab),\n",
        "      use better preprocessing for the metrics,\n",
        "      use more data (from the new datasets - but beware of overfitting),\n",
        "      ...\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import difflib\n",
        "\n",
        "#model = None # TODO: Replace with your model (reuse the code from the previous exercise)\n",
        "input_features = best_correlations # TODO: Replace with the input features (list of strings) you used for the model (e.g., the best_correlations)\n",
        "mse_list = []\n",
        "\n",
        "def get_mse(df, input_features, worker_id, model):\n",
        "  # Get the power column for this dataset\n",
        "  target_word = 'kepler package joules total dynamic'\n",
        "  closest_matches = difflib.get_close_matches(target_word, df.columns, n = 1,  cutoff=0.05)\n",
        "\n",
        "  # Convert counter to gauge\n",
        "  ts = df[\"timestamp\"]\n",
        "  interval = ts[1] - ts[0]\n",
        "  power = df[closest_matches[0]].diff() / interval\n",
        "  y = power\n",
        "\n",
        "  # Change the worker name in the columns, otherwise X will be empty (i.e., worker2 has no data for worker1)\n",
        "  for i in range(2,6):\n",
        "    df.columns = [x.replace(f\"worker{i}\", \"worker1\") for x in df.columns]\n",
        "  X = df[input_features]\n",
        "  # Get predicted power using the model\n",
        "  y_pred = model.predict(X)\n",
        "  # Get rid of NaNs to avoid errors\n",
        "  X = X.fillna(0)\n",
        "  y = y.fillna(0)\n",
        "  # Compare predicted power to power from RAPL\n",
        "  mse = mean_squared_error(y, y_pred)\n",
        "  print(f\"Mean Squared Error (worker{i}):\", mse)\n",
        "  return mse\n",
        "\n",
        "\n",
        "for i in range(1, 6):\n",
        "  # Get the dataset\n",
        "  #worker_df = pd.read_feather(f\"/content/mbps4_worker{i}.feather\")\n",
        "  worker_df = evaluation_dfs[i-1]\n",
        "\n",
        "  # Remove the static columns from the dataframe (hopefully makes dealing with the dataset faster)\n",
        "  unique_counts = worker_df.nunique()\n",
        "  static_columns = unique_counts[unique_counts <= 2].index\n",
        "  worker_df = worker_df.drop(static_columns, axis=1)\n",
        "\n",
        "  # Compute mse\n",
        "  mse = get_mse(worker_df, input_features, i, model)\n",
        "  mse_list.append(mse)\n",
        "\n",
        "print(mse_list)\n",
        "print(f\"Mean MSE: {sum(mse_list) / len(mse_list)} Watts\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YKLRYnjA3i5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What next?\n",
        "\n",
        "For next step, you can try to evaluate your model against previously unseen data from other datasets:\n",
        "\n",
        "- List of datasets here\n",
        "\n",
        "You can also try to improve the model by using more advanced approaches:\n",
        "\n",
        "- Use something other that LinearRegression\n",
        "- Use AutoML\n",
        "- Do additional filtering and preprocessing of features"
      ],
      "metadata": {
        "id": "WQuYJZRRTYBz"
      }
    }
  ]
}